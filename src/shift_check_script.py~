#!/usr/bin/env python
"""
Signal Shift Validation Script

This script traces signal generation and shifting through the entire backtesting pipeline
to identify potential lookahead bias issues.
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Add project directory to path to import modules
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# Import backtester modules
from config import Config
from data import DataHandler
from strategy import StrategyFactory
from backtester import Backtester
from rules import Rule0, Rule1  # Import specific rules for testing
import ga

def trace_single_rule_signals(data, param, rule_func, verbose=True):
    """
    Trace signal generation and shifting for a single rule.
    
    Args:
        data: OHLC data
        param: Parameters for the rule
        rule_func: Rule function to test
        verbose: Whether to print detailed output
    
    Returns:
        Dictionary with original and shifted signals
    """
    if verbose:
        print(f"\n{'='*80}")
        print(f"TRACING RULE: {rule_func.__name__}")
        print(f"{'='*80}")
    
    # Extract a small sample for readability
    sample_data = data.iloc[-30:].copy()
    
    # Call rule function
    score, signals = rule_func(param, sample_data)
    
    # Calculate returns
    log_returns = np.log(sample_data['Close'] / sample_data['Close'].shift(1)).fillna(0)
    
    # Calculate strategy returns WITHOUT shifting (WRONG - would have lookahead bias)
    unshifted_returns = signals * log_returns
    
    # Calculate strategy returns WITH shifting (CORRECT - avoids lookahead bias)
    shifted_signals = signals.shift(1).fillna(0)
    shifted_returns = shifted_signals * log_returns
    
    # Print results for a few key rows
    if verbose:
        print("\nSAMPLE SIGNAL TRACING:")
        print(f"{'Date':^12} | {'Close':^10} | {'Log Return':^10} | {'Signal':^8} | {'Shifted':^8} | {'Unshifted R':^12} | {'Shifted R':^12}")
        print(f"{'-'*12} | {'-'*10} | {'-'*10} | {'-'*8} | {'-'*8} | {'-'*12} | {'-'*12}")
        
        for i in range(min(10, len(sample_data))):
            idx = sample_data.index[i]
            print(f"{str(idx.date()):12} | "
                  f"{sample_data['Close'].iloc[i]:10.2f} | "
                  f"{log_returns.iloc[i]:10.6f} | "
                  f"{signals.iloc[i]:8.0f} | "
                  f"{shifted_signals.iloc[i]:8.0f} | "
                  f"{unshifted_returns.iloc[i]:12.6f} | "
                  f"{shifted_returns.iloc[i]:12.6f}")
        
        # Calculate performance metrics for both approaches
        unshifted_total = unshifted_returns.sum()
        shifted_total = shifted_returns.sum()
        
        print(f"\nUnshifted Returns Total: {unshifted_total:.6f}")
        print(f"Shifted Returns Total: {shifted_total:.6f}")
        print(f"Difference: {unshifted_total - shifted_total:.6f}")
        
        if abs(unshifted_total - shifted_total) > 0.0001:
            print("ALERT: Significant difference between shifted and unshifted returns!")
            print("This suggests proper shifting is important for this rule.")
        else:
            print("NOTE: Little difference between shifted and unshifted for this sample period.")
    
    return {
        'original_signals': signals,
        'shifted_signals': shifted_signals,
        'log_returns': log_returns,
        'unshifted_returns': unshifted_returns,
        'shifted_returns': shifted_returns
    }

def trace_strategy_signals(strategy, data):
    """
    Trace signal generation and shifting within a strategy.
    
    Args:
        strategy: Strategy object
        data: OHLC data
    """
    print(f"\n{'='*80}")
    print(f"TRACING STRATEGY: {strategy}")
    print(f"{'='*80}")
    
    # Generate signals using the strategy
    signals_df = strategy.generate_signals(data)
    
    # Extract a small sample for readability
    sample_data = data.iloc[-30:].copy()
    sample_signals = signals_df.loc[sample_data.index].copy()
    
    print("\nSTRATEGY SIGNAL TRACING:")
    print("Signal counts in strategy output:")
    print(sample_signals['Signal'].value_counts())
    
    # Check if signals are already shifted in the strategy output
    signals = sample_signals['Signal']
    log_returns = sample_signals['LogReturn']
    
    # Create both shifted and unshifted versions for comparison
    shifted_signals = signals.shift(1).fillna(0)
    
    unshifted_returns = signals * log_returns
    shifted_returns = shifted_signals * log_returns
    
    print("\nSAMPLE STRATEGY SIGNALS:")
    print(f"{'Date':^12} | {'Signal':^8} | {'Shifted':^8} | {'LogReturn':^10} | {'Unshifted R':^12} | {'Shifted R':^12}")
    print(f"{'-'*12} | {'-'*8} | {'-'*8} | {'-'*10} | {'-'*12} | {'-'*12}")
    
    for i in range(min(10, len(sample_signals))):
        idx = sample_signals.index[i]
        print(f"{str(idx.date()):12} | "
              f"{signals.iloc[i]:8.0f} | "
              f"{shifted_signals.iloc[i]:8.0f} | "
              f"{log_returns.iloc[i]:10.6f} | "
              f"{unshifted_returns.iloc[i]:12.6f} | "
              f"{shifted_returns.iloc[i]:12.6f}")
    
    # Calculate performance metrics for both approaches
    unshifted_total = unshifted_returns.sum()
    shifted_total = shifted_returns.sum()
    
    print(f"\nUnshifted Returns Total: {unshifted_total:.6f}")
    print(f"Shifted Returns Total: {shifted_total:.6f}")
    print(f"Difference: {unshifted_total - shifted_total:.6f}")
    
    return {
        'strategy_signals': signals,
        'shifted_strategy_signals': shifted_signals,
        'log_returns': log_returns,
        'unshifted_returns': unshifted_returns,
        'shifted_returns': shifted_returns
    }

def trace_backtester_execution(config, data_handler, strategy):
    """
    Trace signal application in the backtester.
    
    Args:
        config: Configuration object
        data_handler: DataHandler object
        strategy: Strategy object
    """
    print(f"\n{'='*80}")
    print(f"TRACING BACKTESTER EXECUTION")
    print(f"{'='*80}")
    
    # Create backtester
    backtester = Backtester(config, data_handler, strategy)
    
    # Get test data
    test_data = data_handler.test_data
    
    # Generate signals on test data
    test_signals = strategy.generate_signals(
        test_data, 
        strategy.rules.rule_params,
        filter_regime=config.filter_regime
    )
    
    # Extract sample for readability
    sample_data = test_data.iloc[-30:].copy()
    sample_signals = test_signals.loc[sample_data.index].copy()
    
    # Create modified signals_df with explicit shift tracking
    modified_signals_df = sample_signals.copy()
    
    # Extract signals and returns
    signals = pd.to_numeric(modified_signals_df['Signal'], errors='coerce')
    returns = pd.to_numeric(modified_signals_df['LogReturn'], errors='coerce')
    
    # Apply shift as in backtester._calculate_performance
    shifted_signals = signals.shift(1).fillna(0)
    
    # Calculate strategy returns
    strategy_returns = shifted_signals * returns
    
    print("\nBACKTESTER SIGNAL APPLICATION:")
    print(f"{'Date':^12} | {'Signal':^8} | {'Shifted':^8} | {'LogReturn':^10} | {'Strategy Return':^16}")
    print(f"{'-'*12} | {'-'*8} | {'-'*8} | {'-'*10} | {'-'*16}")
    
    for i in range(min(10, len(modified_signals_df))):
        idx = modified_signals_df.index[i]
        print(f"{str(idx.date()):12} | "
              f"{signals.iloc[i]:8.0f} | "
              f"{shifted_signals.iloc[i]:8.0f} | "
              f"{returns.iloc[i]:10.6f} | "
              f"{strategy_returns.iloc[i]:16.6f}")
    
    # Calculate performance metrics
    total_return = strategy_returns.sum()
    
    print(f"\nTotal Return in Backtester: {total_return:.6f}")
    
    return {
        'backtester_signals': signals,
        'shifted_backtester_signals': shifted_signals,
        'returns': returns,
        'strategy_returns': strategy_returns
    }

def validate_whole_pipeline():
    """
    Validate the entire signal generation and application pipeline.
    """
    print("\nSIGNAL SHIFTING VALIDATION SCRIPT")
    print("=" * 80)
    
    # Create configuration
    config = Config()
    data_file = '../data/data.csv'  # Update to your data path
    config.data_file = data_file
    config.train_size = 0.7
    config.top_n = 5
    config.use_weights = True
    config.train = True
    config.test = True
    
    # Load data
    data_handler = DataHandler(config)
    data_handler.load_data()
    data_handler.preprocess()
    data_handler.split_data()
    
    # Create strategy
    strategy = StrategyFactory.create_strategy(config)
    
    # Trace individual rules
    rule0_params = (5, 50)  # Example parameters for Rule0
    rule0_results = trace_single_rule_signals(data_handler.data, rule0_params, Rule0)
    
    rule1_params = (14, 30, 70)  # Example parameters for Rule1
    rule1_results = trace_single_rule_signals(data_handler.data, rule1_params, Rule1)
    
    # Train strategy if needed
    print("\nTraining strategy...")
    train_data = data_handler.train_data
    strategy.train(train_data, ga_module=ga)
    
    # Trace strategy signals
    strategy_results = trace_strategy_signals(strategy, data_handler.test_data)
    
    # Trace backtester execution
    backtester_results = trace_backtester_execution(config, data_handler, strategy)
    
    # Evaluate overall pipeline
    print("\nOVERALL SIGNAL SHIFTING VALIDATION:")
    print("=" * 80)
    
    # Check if rule signals are being properly shifted internally
    rule0_diff = rule0_results['unshifted_returns'].sum() - rule0_results['shifted_returns'].sum()
    rule1_diff = rule1_results['unshifted_returns'].sum() - rule1_results['shifted_returns'].sum()
    
    print(f"Rule0 shifted vs. unshifted difference: {rule0_diff:.6f}")
    print(f"Rule1 shifted vs. unshifted difference: {rule1_diff:.6f}")
    
    # Check if strategy signals are being properly shifted
    strategy_diff = strategy_results['unshifted_returns'].sum() - strategy_results['shifted_returns'].sum()
    print(f"Strategy shifted vs. unshifted difference: {strategy_diff:.6f}")
    
    # Determine if signals are being double-shifted
    # Compare backtester and strategy returns
    if 'strategy_returns' in backtester_results and 'shifted_returns' in strategy_results:
        backtester_returns = backtester_results['strategy_returns']
        strategy_shifted_returns = strategy_results['shifted_returns']
        
        # Get common indices
        common_indices = backtester_returns.index.intersection(strategy_shifted_returns.index)
        
        if len(common_indices) > 0:
            backtester_sample = backtester_returns.loc[common_indices]
            strategy_sample = strategy_shifted_returns.loc[common_indices]
            
            print("\nComparing strategy and backtester returns:")
            print(f"Strategy shifted returns sum: {strategy_sample.sum():.6f}")
            print(f"Backtester returns sum: {backtester_sample.sum():.6f}")
            
            is_double_shifted = abs(strategy_sample.sum() - backtester_sample.sum()) > 0.0001
            print(f"Potential double-shifting detected: {is_double_shifted}")
    
    # Generate recommendation
    print("\nRECOMMENDATION:")
    if abs(rule0_diff) > 0.0001 or abs(rule1_diff) > 0.0001:
        print("✓ Rules are correctly applying shifting for internal evaluation")
    else:
        print("✗ Rules may not be correctly shifting signals for evaluation")
    
    if abs(strategy_diff) > 0.0001:
        print("✓ Strategy signals show impact of shifting (expected)")
    else:
        print("? Strategy signals show minimal impact from shifting (unusual)")
    
    if 'is_double_shifted' in locals() and is_double_shifted:
        print("! WARNING: Potential double-shifting detected between strategy and backtester")
        print("  This may indicate that signals are being shifted multiple times")
    else:
        print("✓ No evidence of double-shifting detected")
    
    return {
        'rule0_results': rule0_results,
        'rule1_results': rule1_results,
        'strategy_results': strategy_results,
        'backtester_results': backtester_results
    }

if __name__ == "__main__":
    validate_whole_pipeline()
